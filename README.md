# SAM 2 Explainer

This repository is a fork of [SAM 2: Segment Anything in Images and Videos](https://github.com/facebookresearch/segment-anything-2) with an added Jupyter notebook that explains the utility functions used in the project.

## Contents

- All original files from the SAM 2 repository
- `utils.ipynb`: A comprehensive Jupyter notebook that breaks down and explains various utility functions used in the SAM 2 project

## Original SAM 2 Project

SAM 2 (Segment Anything Model 2) is a foundation model for promptable visual segmentation in images and videos. It extends the original SAM to video by considering images as a video with a single frame. The model uses a simple transformer architecture with streaming memory for real-time video processing.

For more information about the original project, please refer to:
- [Original Repository](https://github.com/facebookresearch/segment-anything-2)
- [Research Paper](https://ai.meta.com/research/publications/sam2-segment-anything-in-images-and-videos/)
- [Project Website](https://segment-anything.com/)
- [Demo](https://segment-anything.com/demo)
- [Dataset](https://ai.meta.com/datasets/segment-anything/)
- [Blog Post](https://ai.meta.com/blog/segment-anything-model-2-foundation-model-segmentation/)

## Notebook Contents

The `utils.ipynb` notebook contains explanations and demonstrations of various utility functions used in the SAM 2 project. It aims to provide a clearer understanding of the codebase and its components.

## Acknowledgments

- Original SAM 2 project by Meta AI, FAIR
- All contributors to the original SAM 2 project
